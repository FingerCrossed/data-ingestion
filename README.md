# Supply Chain Ingestion Pipeline

This project simulates a scalable ingestion pipeline for global supply chain data. It covers ingestion, validation, and observability across the full path from raw CSV/JSON files to a data warehouse.

---

## ğŸ”§ Tech Stack

- **Kafka** for streaming ingestion
- **Apache Spark (PySpark)** for ETL transformation
- **Apache Airflow** for orchestration and SLA monitoring
- **Snowflake** as the data warehouse
- **FastAPI** for exposing lineage and SLA metadata
- **Databricks** for local notebook testing and job runs

---

## ğŸ“ Project Structure


supply-chain-ingestion-pipeline/

â”œâ”€â”€ dags/                   â† Airflow DAGs

â”œâ”€â”€ kafka_producer/        â† Kafka producer scripts

â”œâ”€â”€ spark_jobs/            â† ETL with PySpark

â”œâ”€â”€ api/                   â† Lineage APIs with FastAPI

â”œâ”€â”€ data/                  â† Sample input files (CSV/JSON)

â”œâ”€â”€ ingestion_log/         â† Log schema, data, and queries

â”œâ”€â”€ config/                â† Region-based config files

â”œâ”€â”€ notebooks/             â† Databricks notebooks for testing


---

## âœ… Features

- Multi-region file-based ingestion (SAP / Teradata to S3)
- Real-time Kafka publishing + ingestion logs
- PySpark validation and deduplication
- SLA breach monitoring via Airflow and Slack
- Lineage API for pipeline health and metadata

---

## ğŸ“¦ Python Requirements


kafka-python
pyspark
fastapi
uvicorn
pydantic
snowflake-connector-python
sqlalchemy
python-dotenv
slack_sdk
